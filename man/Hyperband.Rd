% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/HyperResNet_HyperXception.R
\name{Hyperband}
\alias{Hyperband}
\title{Tool for searching the best hyperparameters for computer vision.}
\usage{
Hyperband(
  hypermodel = NULL,
  optimizer = NULL,
  loss = NULL,
  metrics = NULL,
  hyperparameters = NULL,
  objective = NULL,
  max_epochs = NULL,
  factor = NULL,
  hyperband_iterations = NULL,
  seed = NULL,
  tune_new_entries = NULL,
  distribution_strategy = NULL,
  directory = NULL,
  project_name = NULL,
  ...
)
}
\arguments{
\item{hypermodel}{Define a model-building function. It takes an argument "hp" from which 
you can sample hyperparameters.}

\item{optimizer}{An optimizer is one of the arguments required for compiling a Keras model}

\item{loss}{A loss function (or objective function, or optimization score function) is one of
the parameters required to compile a model}

\item{metrics}{A metric is a function that is used to judge the performance of your model}

\item{hyperparameters}{HyperParameters class instance. Can be used to override (or register in advance) 
hyperparamters in the search space.}

\item{objective}{A loss metrics function for tracking the model performance e.g. "val_precision". The name of 
the objective to optimize (whether to minimize or maximize is automatically inferred for built-in metrics)}

\item{max_epochs}{to train the model. Note that in conjunction with initial_epoch, 
epochs is to be understood as "final epoch". The model is not trained for a number of iterations
given by epochs, but merely until the epoch of index epochs is reached.}

\item{factor}{Int. Reduction factor for the number of epochs and number of models for each bracket.}

\item{hyperband_iterations}{Int >= 1. The number of times to iterate over the full Hyperband algorithm.
One iteration will run approximately ```max_epochs * (math.log(max_epochs, factor) ** 2)``` cumulative epochs
across all trials. It is recommended to set this to as high a value as is within your resource budget.}

\item{seed}{Int. Random seed.}

\item{tune_new_entries}{Whether hyperparameter entries that are requested by the hypermodel 
but that were not specified in hyperparameters should be added to the search space, or not. If not, 
then the default value for these parameters will be used.}

\item{distribution_strategy}{Scale up from running single-threaded locally to running on dozens or 
hundreds of workers in parallel. Distributed Keras Tuner uses a chief-worker model. The chief runs a 
service to which the workers report results and query for the hyperparameters to try next. The chief 
should be run on a single-threaded CPU instance (or alternatively as a separate process on 
one of the workers). Keras Tuner also supports data parallelism via tf.distribute. 
Data parallelism and distributed tuning can be combined. For example, if you have 10 workers 
with 4 GPUs on each worker, you can run 10 parallel trials with each trial training on 4 GPUs 
by using tf.distribute.MirroredStrategy. You can also run each trial on TPUs 
via tf.distribute.experimental.TPUStrategy. Currently tf.distribute.MultiWorkerMirroredStrategy 
is not supported, but support for this is on the roadmap.}

\item{directory}{The dir where training logs are stored}

\item{project_name}{Detailed logs, checkpoints, etc, in the folder my_dir/helloworld, i.e. 
directory/project_name.}

\item{...}{Some additional arguments}
}
\description{
Tool for searching the best hyperparameters for computer vision.
}
