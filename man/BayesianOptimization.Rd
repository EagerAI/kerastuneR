% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bayesian_optimisation.R
\name{BayesianOptimization}
\alias{BayesianOptimization}
\title{BayesianOptimization}
\usage{
BayesianOptimization(
  objective,
  max_trials,
  num_initial_points = NULL,
  alpha = 1e-04,
  beta = 2.6,
  seed = NULL,
  hyperparameters = NULL,
  allow_new_entries = TRUE,
  tune_new_entries = TRUE
)
}
\arguments{
\item{objective}{String or `kerastuner.Objective`. If a string, the direction of the optimization (min or max) will be inferred.}

\item{max_trials}{Int. Total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before `max_trial` models have been tested if the search space has been exhausted.}

\item{num_initial_points}{(Optional) Int. The number of randomly generated samples as initial training data for Bayesian optimization. If not specified, a value of 3 times the dimensionality of the hyperparameter space is used.}

\item{alpha}{Float. Value added to the diagonal of the kernel matrix during fitting. It represents the expected amount of noise in the observed performances in Bayesian optimization.}

\item{beta}{Float. The balancing factor of exploration and exploitation. The larger it is, the more explorative it is.}

\item{seed}{Int. Random seed.}

\item{hyperparameters}{HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space.}

\item{allow_new_entries}{Whether the hypermodel is allowed to request hyperparameter entries not listed in `hyperparameters`.}

\item{tune_new_entries}{Whether hyperparameter entries that are requested by the hypermodel but that were not specified in `hyperparameters` should be added to the search space, or not. If not, then the default value for these parameters will be used.}
}
\description{
Bayesian optimization oracle.
}
\details{
It uses Bayesian optimization with a underlying Gaussian process model.
The acquisition function used is upper confidence bound (UCB), which can
be found in the following link:
https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf # Arguments objective: String or `kerastuner.Objective`. If a string, the direction of the optimization (min or max) will be inferred. max_trials: Int. Total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before `max_trial` models have been tested if the search space has been exhausted. num_initial_points: (Optional) Int. The number of randomly generated samples as initial training data for Bayesian optimization. If not specified, a value of 3 times the dimensionality of the hyperparameter space is used. alpha: Float. Value added to the diagonal of the kernel matrix during fitting. It represents the expected amount of noise in the observed performances in Bayesian optimization. beta: Float. The balancing factor of exploration and exploitation. The larger it is, the more explorative it is. seed: Int. Random seed. hyperparameters: HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries: Whether hyperparameter entries that are requested by the hypermodel but that were not specified in `hyperparameters` should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries: Whether the hypermodel is allowed to request hyperparameter entries not listed in `hyperparameters`.
}
\section{be found in the following link}{

https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf
}

\examples{

\dontrun{
# The usage of 'tf$keras'
library(keras)
library(dplyr)
library(kerastuneR)
tf$keras$Input(shape=list(28L, 28L, 1L))
}

}
