<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Interface to Keras Tuner  • kerastuneR</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="pkgdown.css" rel="stylesheet">
<script src="pkgdown.js"></script><meta property="og:title" content="Interface to Keras Tuner ">
<meta property="og:description" content="Keras Tuner is a hypertuning framework made for humans. 
             It aims at making the life of AI practitioners, hypertuner 
             algorithm creators and model designers as simple as possible by 
             providing them with a clean and easy to use API for hypertuning. 
             Keras Tuner makes moving from a base model to a hypertuned one quick and 
             easy by only requiring you to change a few lines of code.">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-home">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="index.html">kerastuneR</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.0.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="articles/BayesianOptimisation.html">Bayesian Optimization</a>
    </li>
    <li>
      <a href="articles/HyperModel_subclass.html">HyperModel subclass</a>
    </li>
    <li>
      <a href="articles/Introduction.html">Introduction to kerastuneR</a>
    </li>
    <li>
      <a href="articles/MNIST.html">MNIST hypertuning</a>
    </li>
  </ul>
</li>
<li>
  <a href="news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/henry090/kerastuneR">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="contents col-md-9">

<div id="r-interface-to-keras-tuner" class="section level2">
<h2 class="hasAnchor">
<a href="#r-interface-to-keras-tuner" class="anchor"></a>R interface to Keras Tuner</h2>
<p>The kerastuneR package provides R wrappers to <a href="https://keras-team.github.io/keras-tuner/">Keras Tuner</a>.</p>
<p>Keras Tuner is a hypertuning framework made for humans. It aims at making the life of AI practitioners, hypertuner algorithm creators and model designers as simple as possible by providing them with a clean and easy to use API for hypertuning. Keras Tuner makes moving from a base model to a hypertuned one quick and easy by only requiring you to change a few lines of code.</p>
<p><img src="images/kerastuneR.png" width="200" align="right" style="margin-left: 15px;" alt="Keras Tuner"></p>
<p><a href="https://github.com/henry090/kerastuneR"><img src="https://github.com/henry090/kerastuneR/workflows/R-CMD/badge.svg" alt="Actions Status"></a> <a href="https://travis-ci.com/henry090/kerastuneR"><img src="https://travis-ci.com/henry090/kerastuneR.svg?branch=master" alt="Build Status"></a> <a href="https://www.tidyverse.org/lifecycle/#experimental"><img src="https://img.shields.io/badge/lifecycle-experimental-orange.svg" alt="Lifecycle: experimental"></a> <a href="https://codecov.io/gh/henry090/kerastuneR?branch=master"><img src="https://codecov.io/gh/henry090/kerastuneR/branch/master/graph/badge.svg" alt="Codecov test coverage"></a></p>
<p>A hyperparameter tuner for <a href="https://keras.io/">Keras</a>, specifically for <code>tf$keras</code> with <em>TensorFlow 2.0</em>.</p>
<p>Full documentation and tutorials available on the <a href="https://henry090.github.io/kerastuneR/">Keras Tuner website</a>.</p>
</div>
<div id="installation" class="section level2">
<h2 class="hasAnchor">
<a href="#installation" class="anchor"></a>Installation</h2>
<p>Requirements:</p>
<ul>
<li>Python 3.6</li>
<li>TensorFlow 2.0</li>
</ul>
<p>Currently, the package is available on github:</p>
<pre><code><a href="https://rdrr.io/pkg/devtools/man/remote-reexports.html">devtools::install_github('henry090/kerastuneR')</a></code></pre>
<p>Later, you need to install the python module kerastuner:</p>
<pre><code><a href="reference/install_kerastuner.html">kerastuneR::install_kerastuner()</a></code></pre>
</div>
<div id="usage-the-basics" class="section level2">
<h2 class="hasAnchor">
<a href="#usage-the-basics" class="anchor"></a>Usage: the basics</h2>
<p>Here’s how to perform hyperparameter tuning for a single-layer dense neural network using random search.</p>
<p>First, we define a model-building function. It takes an argument <code>hp</code> from which you can sample hyperparameters, such as <code>hp$Int('units', min_value = 32, max_value = 512, step = 32)</code> (an integer from a certain range).</p>
<p>Sample data:</p>
<pre><code>x_data &lt;- matrix(data = runif(500,0,1),nrow = 50,ncol = 5)
y_data &lt;-  ifelse(runif(50,0,1) &gt; 0.6, 1L,0L) %&gt;% as.matrix()

x_data2 &lt;- matrix(data = runif(500,0,1),nrow = 50,ncol = 5)
y_data2 &lt;-  ifelse(runif(50,0,1) &gt; 0.6, 1L,0L) %&gt;% as.matrix()</code></pre>
<p>This function returns a compiled model.</p>
<pre><code>library(keras)
library(kerastuneR)
library(dplyr)

build_model = function(hp) {
  
  model = keras_model_sequential()
  model %&gt;% layer_dense(units = hp$Int('units',
                                     min_value = 32,
                                     max_value = 512,
                                     step=  32),input_shape = ncol(x_data),
                        activation =  'relu') %&gt;%
    layer_dense(units = 1, activation = 'softmax') %&gt;%
    compile(
      optimizer = tf$keras$optimizers$Adam(
        hp$Choice('learning_rate',
                  values=c(1e-2, 1e-3, 1e-4))),
      loss = 'binary_crossentropy',
      metrics = 'accuracy')
  return(model)
}</code></pre>
<p>Next, instantiate a tuner. You should specify the model-building function, the name of the objective to optimize (whether to minimize or maximize is automatically inferred for built-in metrics), the total number of trials <code>(max_trials)</code> to test, and the number of models that should be built and fit for each trial <code>(executions_per_trial)</code>.</p>
<p>Available tuners are <code>RandomSearch</code> and <code>Hyperband</code>.</p>
<blockquote>
<p>Note: the purpose of having multiple executions per trial is to reduce results variance and therefore be able to more accurately assess the performance of a model. If you want to get results faster, you could set executions_per_trial=1 (single round of training for each model configuration).</p>
</blockquote>
<pre><code>tuner = RandomSearch(
    build_model,
    objective = 'val_accuracy',
    max_trials = 5,
    executions_per_trial = 3,
    directory = 'my_dir',
    project_name = 'helloworld')</code></pre>
<p>You can print a summary of the search space:</p>
<pre><code>tuner %&gt;% search_summary()</code></pre>
<p>Then, start the search for the best hyperparameter configuration. The call to search has the same signature as <code>model %&gt;% fit()</code>. But here instead of <code>fit()</code> we call <code><a href="reference/fit_tuner.html">fit_tuner()</a></code>.</p>
<pre><code>tuner %&gt;% fit_tuner(x_data,y_data,
                    epochs = 5, 
                    validation_data = list(x_data2,y_data2))</code></pre>
<div id="plot-results" class="section level3">
<h3 class="hasAnchor">
<a href="#plot-results" class="anchor"></a>Plot results</h3>
<p>There is a function <code>plot_tuner</code> which allows user to plot the search results. For this purpose, we used the parallel coordinates plot from <code>plotly</code>. This function allows to get a data.frame of the results, as well.</p>
<pre><code>result = kerastuneR::plot_tuner(tuner)
# the list will show the plot and the data.frame
result </code></pre>
<p><img src="images/tuner.gif" width="900" align="center" style="margin-left: 15px;" alt="Keras Tuner plot"></p>
</div>
</div>
<div id="you-can-easily-restrict-the-search-space-to-just-a-few-parameters" class="section level2">
<h2 class="hasAnchor">
<a href="#you-can-easily-restrict-the-search-space-to-just-a-few-parameters" class="anchor"></a>You can easily restrict the search space to just a few parameters</h2>
<p>If you have an existing hypermodel, and you want to search over only a few parameters (such as the learning rate), you can do so by passing a <code>hyperparameters</code> argument to the tuner constructor, as well as <code>tune_new_entries=FALSE</code> to specify that parameters that you didn’t list in <code>hyperparameters</code> should not be tuned. For these parameters, the default value gets used.</p>
<pre><code>library(keras)
library(kerastuneR)
library(dplyr)

mnist_data = dataset_fashion_mnist()
c(mnist_train, mnist_test) %&lt;-%  mnist_data
rm(mnist_data)

mnist_train$x = tf$dtypes$cast(mnist_train$x, 'float32') / 255.
mnist_test$x = tf$dtypes$cast(mnist_test$x, 'float32') / 255.

mnist_train$x = keras::k_reshape(mnist_train$x,shape = c(6e4,28,28))
mnist_test$x = keras::k_reshape(mnist_test$x,shape = c(1e4,28,28))


hp = HyperParameters()
hp$Choice('learning_rate', c(1e-1, 1e-3))
hp$Int('num_layers', 2L, 20L)


mnist_model = function(hp) {
  
  model = keras_model_sequential() %&gt;% 
    layer_flatten(input_shape = c(28,28))
  for (i in 1:(hp$get('num_layers')) ) {
    model %&gt;% layer_dense(32, activation='relu') %&gt;% 
      layer_dense(units = 10, activation='softmax')
  } %&gt;% 
    compile(
      optimizer = tf$keras$optimizers$Adam(hp$get('learning_rate')),
      loss = 'sparse_categorical_crossentropy',
      metrics = 'accuracy') 
  return(model)
  
}


tuner = RandomSearch(
  hypermodel =  mnist_model,
  max_trials = 5,
  hyperparameters = hp,
  tune_new_entries = T,
  objective = 'val_accuracy',
  directory = 'dir_1',
  project_name = 'mnist_space')

tuner %&gt;% fit_tuner(x = mnist_train$x,
                    y = mnist_train$y,
                    epochs = 5,
                    validation_data = list(mnist_test$x, mnist_test$y))

</code></pre>
</div>
<div id="you-can-use-a-hypermodel-subclass-instead-of-a-model-building-function" class="section level2">
<h2 class="hasAnchor">
<a href="#you-can-use-a-hypermodel-subclass-instead-of-a-model-building-function" class="anchor"></a>You can use a HyperModel subclass instead of a model-building function</h2>
<p>This makes it easy to share and reuse hypermodels.</p>
<p>A <code>HyperModel</code> subclass only needs to implement a <code><a href="https://rdrr.io/r/utils/PkgUtils.html">build(self, hp)</a></code> method.</p>
<pre><code>library(keras)
library(tensorflow)
library(dplyr)
library(kerastuneR)

x_data &lt;- matrix(data = runif(500,0,1),nrow = 50,ncol = 5)
y_data &lt;- ifelse(runif(50,0,1) &gt; 0.6, 1L,0L) %&gt;% as.matrix()

x_data2 &lt;- matrix(data = runif(500,0,1),nrow = 50,ncol = 5)
y_data2 &lt;- ifelse(runif(50,0,1) &gt; 0.6, 1L,0L) %&gt;% as.matrix()


HyperModel &lt;- reticulate::PyClass(
  'HyperModel',
  inherit = kerastuneR::HyperModel_class(),
  list(
    
    `__init__` = function(self, num_classes) {
      
      self$num_classes = num_classes
      NULL
    },
    build = function(self,hp) {
      model = keras_model_sequential() 
      model %&gt;% layer_dense(units = hp$Int('units',
                                           min_value = 32,
                                           max_value = 512,
                                           step = 32),
                            input_shape = ncol(x_data),
                            activation = 'relu') %&gt;% 
        layer_dense(as.integer(self$num_classes), activation = 'softmax') %&gt;% 
        compile(
          optimizer = tf$keras$optimizers$Adam(
            hp$Choice('learning_rate',
                      values = c(1e-2, 1e-3, 1e-4))),
          loss = 'sparse_categorical_crossentropy',
          metrics = 'accuracy')
    }
  )
)

hypermodel = HyperModel(num_classes = 10)


tuner = RandomSearch(hypermodel = hypermodel,
                      objective = 'val_accuracy',
                      max_trials = 2,
                      executions_per_trial = 1,
                      directory = 'my_dir5',
                      project_name = 'helloworld')
</code></pre>
<p>Documentation, advanced model tuning, and tutorials can be found on <a href="https://henry090.github.io/kerastuneR/" class="uri">https://henry090.github.io/kerastuneR/</a></p>
</div>

  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <div class="links">
<h2>Links</h2>
<ul class="list-unstyled">
<li>Browse source code at <br><a href="https://github.com/henry090/kerastuneR">https://​github.com/​henry090/​kerastuneR</a>
</li>
<li>Report a bug at <br><a href="https://github.com/henry090/kerastuneR/issues">https://​github.com/​henry090/​kerastuneR/​issues</a>
</li>
</ul>
</div>
<div class="license">
<h2>License</h2>
<ul class="list-unstyled">
<li>Apache License 2.0</li>
</ul>
</div>
<div class="developers">
<h2>Developers</h2>
<ul class="list-unstyled">
<li>Turgut Abdullayev <br><small class="roles"> Author, maintainer </small>  </li>
<li><a href="authors.html">All authors...</a></li>
</ul>
</div>

  </div>
</div>


      <footer><div class="copyright">
  <p>Developed by Turgut Abdullayev.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.4.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
