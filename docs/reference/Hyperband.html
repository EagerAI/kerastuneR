<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Hyperband — Hyperband • kerastuneR</title><!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous"><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css"><script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../pkgdown.js"></script><meta property="og:title" content="Hyperband — Hyperband"><meta property="og:description" content="Variation of HyperBand algorithm."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body data-spy="scroll" data-target="#toc">
    

    <div class="container template-reference-topic">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">kerastuneR</a>
<<<<<<< HEAD
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.0.5</span>
=======
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.0.6</span>
>>>>>>> e5e5fc418dff9176842e75a405470546a3342c42
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu"><li>
      <a href="../articles/BayesianOptimisation.html">Bayesian Optimization</a>
    </li>
    <li>
      <a href="../articles/HyperModel_subclass.html">HyperModel subclass</a>
    </li>
    <li>
      <a href="../articles/Introduction.html">Introduction to kerastuneR</a>
    </li>
    <li>
      <a href="../articles/MNIST.html">MNIST hypertuning</a>
    </li>
    <li>
      <a href="../articles/best_practice.html">KerasTuner best practices</a>
    </li>
  </ul></li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"><li>
  <a href="https://github.com/EagerAI/kerastuneR/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Hyperband</h1>
    <small class="dont-index">Source: <a href="https://github.com/EagerAI/kerastuneR/blob/HEAD/R/HyperResNet_HyperXception.R" class="external-link"><code>R/HyperResNet_HyperXception.R</code></a></small>
    <div class="hidden name"><code>Hyperband.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>Variation of HyperBand algorithm.</p>
    </div>

    <div id="ref-usage">
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">Hyperband</span><span class="op">(</span></span>
<<<<<<< HEAD
<span>  <span class="va">hypermodel</span>,</span>
<span>  optimizer <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  loss <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  metrics <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  hyperparameters <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  <span class="va">objective</span>,</span>
<span>  <span class="va">max_epochs</span>,</span>
<span>  factor <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  hyperband_iterations <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  seed <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  tune_new_entries <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  allow_new_entries <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  distribution_strategy <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  directory <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  project_name <span class="op">=</span> <span class="cn">NULL</span>,</span>
=======
<span>  hypermodel <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  objective <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  max_epochs <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  factor <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  hyperband_iterations <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  seed <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  hyperparameters <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  tune_new_entries <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  allow_new_entries <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  max_retries_per_trial <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  max_consecutive_failed_trials <span class="op">=</span> <span class="fl">3</span>,</span>
>>>>>>> e5e5fc418dff9176842e75a405470546a3342c42
<span>  <span class="va">...</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div id="arguments">
    <h2>Arguments</h2>
    <dl><dt>hypermodel</dt>
<<<<<<< HEAD
<dd><p>Define a model-building function. It takes an argument "hp" from which 
you can sample hyperparameters.</p></dd>


<dt>optimizer</dt>
<dd><p>An optimizer is one of the arguments required for compiling a Keras model</p></dd>


<dt>loss</dt>
<dd><p>A loss function (or objective function, or optimization score function) is one of
the parameters required to compile a model</p></dd>


<dt>metrics</dt>
<dd><p>A metric is a function that is used to judge the performance of your model</p></dd>


<dt>hyperparameters</dt>
<dd><p>HyperParameters class instance. Can be used to override (or register in advance) 
hyperparamters in the search space.</p></dd>


<dt>objective</dt>
<dd><p>A loss metrics function for tracking the model performance e.g. "val_precision". The name of 
the objective to optimize (whether to minimize or maximize is automatically inferred for built-in metrics)</p></dd>


<dt>max_epochs</dt>
<dd><p>to train the model. Note that in conjunction with initial_epoch, 
epochs is to be understood as "final epoch". The model is not trained for a number of iterations
given by epochs, but merely until the epoch of index epochs is reached.</p></dd>


<dt>factor</dt>
<dd><p>Int. Reduction factor for the number of epochs and number of models for each bracket.</p></dd>


<dt>hyperband_iterations</dt>
<dd><p>Int &gt;= 1. The number of times to iterate over the full Hyperband algorithm.
One iteration will run approximately ```max_epochs * (math.log(max_epochs, factor) ** 2)``` cumulative epochs
across all trials. It is recommended to set this to as high a value as is within your resource budget.</p></dd>


<dt>seed</dt>
<dd><p>Int. Random seed.</p></dd>


<dt>tune_new_entries</dt>
<dd><p>Whether hyperparameter entries that are requested by the hypermodel 
but that were not specified in hyperparameters should be added to the search space, or not. 
If not, then the default value for these parameters will be used.</p></dd>


<dt>allow_new_entries</dt>
<dd><p>Whether the hypermodel is allowed to request hyperparameter entries not listed in 
`hyperparameters`. **kwargs: Keyword arguments relevant to all `Tuner` subclasses. Please see the docstring for `Tuner`.</p></dd>


<dt>distribution_strategy</dt>
<dd><p>Scale up from running single-threaded locally to running on dozens or 
hundreds of workers in parallel. Distributed Keras Tuner uses a chief-worker model. The chief runs a 
service to which the workers report results and query for the hyperparameters to try next. The chief 
should be run on a single-threaded CPU instance (or alternatively as a separate process on 
one of the workers). Keras Tuner also supports data parallelism via tf.distribute. 
Data parallelism and distributed tuning can be combined. For example, if you have 10 workers 
with 4 GPUs on each worker, you can run 10 parallel trials with each trial training on 4 GPUs 
by using tf.distribute.MirroredStrategy. You can also run each trial on TPUs 
via tf.distribute.experimental.TPUStrategy. Currently tf.distribute.MultiWorkerMirroredStrategy 
is not supported, but support for this is on the roadmap.</p></dd>


<dt>directory</dt>
<dd><p>The dir where training logs are stored</p></dd>


<dt>project_name</dt>
<dd><p>Detailed logs, checkpoints, etc, in the folder my_dir/helloworld, i.e. 
directory/project_name.</p></dd>
=======
<dd><p>Instance of `HyperModel` class (or callable that takes hyperparameters and returns a `Model` instance). It is optional when `Tuner.run_trial()` is overriden and does not use `self.hypermodel`.</p></dd>


<dt>objective</dt>
<dd><p>A string, `keras_tuner.Objective` instance, or a list of `keras_tuner.Objective`s and strings. If a string, the direction of the optimization (min or max) will be inferred. If a list of `keras_tuner.Objective`, we will minimize the sum of all the objectives to minimize subtracting the sum of all the objectives to maximize. The `objective` argument is optional when `Tuner.run_trial()` or `HyperModel.fit()` returns a single float as the objective to minimize.</p></dd>


<dt>max_epochs</dt>
<dd><p>Integer, the maximum number of epochs to train one model. It is recommended to set this to a value slightly higher than the expected epochs to convergence for your largest Model, and to use early stopping during training (for example, via `tf.keras.callbacks.EarlyStopping`). Defaults to 100.</p></dd>


<dt>factor</dt>
<dd><p>Integer, the reduction factor for the number of epochs and number of models for each bracket. Defaults to 3.</p></dd>


<dt>hyperband_iterations</dt>
<dd><p>Integer, at least 1, the number of times to iterate over the full Hyperband algorithm. One iteration will run approximately `max_epochs * (math.log(max_epochs, factor) ** 2)` cumulative epochs across all trials. It is recommended to set this to as high a value as is within your resource budget. Defaults to 1.</p></dd>


<dt>seed</dt>
<dd><p>Optional integer, the random seed.</p></dd>


<dt>hyperparameters</dt>
<dd><p>Optional HyperParameters instance. Can be used to override (or register in advance) hyperparameters in the search space.</p></dd>


<dt>tune_new_entries</dt>
<dd><p>Boolean, whether hyperparameter entries that are requested by the hypermodel but that were not specified in `hyperparameters` should be added to the search space, or not. If not, then the default value for these parameters will be used. Defaults to TRUE.</p></dd>


<dt>allow_new_entries</dt>
<dd><p>Boolean, whether the hypermodel is allowed to request hyperparameter entries not listed in `hyperparameters`. Defaults to TRUE.</p></dd>


<dt>max_retries_per_trial</dt>
<dd><p>Integer. Defaults to 0. The maximum number of times to retry a `Trial` if the trial crashed or the results are invalid.</p></dd>


<dt>max_consecutive_failed_trials</dt>
<dd><p>Integer. Defaults to 3. The maximum number of consecutive failed `Trial`s. When this number is reached, the search will be stopped. A `Trial` is marked as failed when none of the retries succeeded. **kwargs: Keyword arguments relevant to all `Tuner` subclasses. Please see the docstring for `Tuner`.</p></dd>
>>>>>>> e5e5fc418dff9176842e75a405470546a3342c42


<dt>...</dt>
<dd><p>Some additional arguments</p></dd>

</dl></div>
    <div id="value">
    <h2>Value</h2>
    

<p>a hyperparameter tuner object Hyperband</p>
    </div>
    <div id="details">
    <h2>Details</h2>
<<<<<<< HEAD
    <p>Reference: Li, Lisha, and Kevin Jamieson. ["Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization." Journal of Machine Learning Research 18 (2018): 1-52]( http://jmlr.org/papers/v18/16-558.html). # Arguments hypermodel: Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance). objective: String. Name of model metric to minimize or maximize, e.g. "val_accuracy". max_epochs: Int. The maximum number of epochs to train one model. It is recommended to set this to a value slightly higher than the expected time to convergence for your largest Model, and to use early stopping during training (for example, via `tf.keras.callbacks.EarlyStopping`). factor: Int. Reduction factor for the number of epochs and number of models for each bracket. hyperband_iterations: Int &gt;= 1. The number of times to iterate over the full Hyperband algorithm. One iteration will run approximately `max_epochs * (math.log(max_epochs, factor) ** 2)` cumulative epochs across all trials. It is recommended to set this to as high a value as is within your resource budget. seed: Int. Random seed. hyperparameters: HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries: Whether hyperparameter entries that are requested by the hypermodel but that were not specified in `hyperparameters` should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries: Whether the hypermodel is allowed to request hyperparameter entries not listed in `hyperparameters`. **kwargs: Keyword arguments relevant to all `Tuner` subclasses. Please see the docstring for `Tuner`.</p>
=======
    <p>Reference: Li, Lisha, and Kevin Jamieson. ["Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization." Journal of Machine Learning Research 18 (2018): 1-52]( http://jmlr.org/papers/v18/16-558.html).</p>
>>>>>>> e5e5fc418dff9176842e75a405470546a3342c42
    </div>
    <div id="reference">
    <h2>Reference</h2>
    

<p>Li, Lisha, and Kevin Jamieson. ["Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization." Journal of Machine Learning Research 18 (2018): 1-52]( http://jmlr.org/papers/v18/16-558.html).</p>
    </div>

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top"><h2 data-toc-skip>Contents</h2>
    </nav></div>
</div>


      <footer><div class="copyright">
  <p></p><p>Developed by Turgut Abdullayev.</p>
</div>

<div class="pkgdown">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer></div>

  


  

  </body></html>

