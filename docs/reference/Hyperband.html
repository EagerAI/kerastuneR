<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Hyperband — Hyperband • kerastuneR</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous" />


<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Hyperband — Hyperband" />
<meta property="og:description" content="Variation of HyperBand algorithm." />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">kerastuneR</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.0.3</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/BayesianOptimisation.html">Bayesian Optimization</a>
    </li>
    <li>
      <a href="../articles/HyperModel_subclass.html">HyperModel subclass</a>
    </li>
    <li>
      <a href="../articles/Introduction.html">Introduction to kerastuneR</a>
    </li>
    <li>
      <a href="../articles/MNIST.html">MNIST hypertuning</a>
    </li>
    <li>
      <a href="../articles/best_practice.html">KerasTuner best practices</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/henry090/kerastuneR/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Hyperband</h1>
    <small class="dont-index">Source: <a href='https://github.com/henry090/kerastuneR/blob/master/R/HyperResNet_HyperXception.R'><code>R/HyperResNet_HyperXception.R</code></a></small>
    <div class="hidden name"><code>Hyperband.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>Variation of HyperBand algorithm.</p>
    </div>

    <pre class="usage"><span class='fu'>Hyperband</span><span class='op'>(</span>
  <span class='va'>hypermodel</span>,
  optimizer <span class='op'>=</span> <span class='cn'>NULL</span>,
  loss <span class='op'>=</span> <span class='cn'>NULL</span>,
  metrics <span class='op'>=</span> <span class='cn'>NULL</span>,
  hyperparameters <span class='op'>=</span> <span class='cn'>NULL</span>,
  <span class='va'>objective</span>,
  <span class='va'>max_epochs</span>,
  factor <span class='op'>=</span> <span class='fl'>3</span>,
  hyperband_iterations <span class='op'>=</span> <span class='fl'>1</span>,
  seed <span class='op'>=</span> <span class='cn'>NULL</span>,
  tune_new_entries <span class='op'>=</span> <span class='cn'>TRUE</span>,
  allow_new_entries <span class='op'>=</span> <span class='cn'>TRUE</span>,
  distribution_strategy <span class='op'>=</span> <span class='cn'>NULL</span>,
  directory <span class='op'>=</span> <span class='cn'>NULL</span>,
  project_name <span class='op'>=</span> <span class='cn'>NULL</span>,
  <span class='va'>...</span>
<span class='op'>)</span></pre>

    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>hypermodel</th>
      <td><p>Define a model-building function. It takes an argument "hp" from which 
you can sample hyperparameters.</p></td>
    </tr>
    <tr>
      <th>optimizer</th>
      <td><p>An optimizer is one of the arguments required for compiling a Keras model</p></td>
    </tr>
    <tr>
      <th>loss</th>
      <td><p>A loss function (or objective function, or optimization score function) is one of
the parameters required to compile a model</p></td>
    </tr>
    <tr>
      <th>metrics</th>
      <td><p>A metric is a function that is used to judge the performance of your model</p></td>
    </tr>
    <tr>
      <th>hyperparameters</th>
      <td><p>HyperParameters class instance. Can be used to override (or register in advance) 
hyperparamters in the search space.</p></td>
    </tr>
    <tr>
      <th>objective</th>
      <td><p>A loss metrics function for tracking the model performance e.g. "val_precision". The name of 
the objective to optimize (whether to minimize or maximize is automatically inferred for built-in metrics)</p></td>
    </tr>
    <tr>
      <th>max_epochs</th>
      <td><p>to train the model. Note that in conjunction with initial_epoch, 
epochs is to be understood as "final epoch". The model is not trained for a number of iterations
given by epochs, but merely until the epoch of index epochs is reached.</p></td>
    </tr>
    <tr>
      <th>factor</th>
      <td><p>Int. Reduction factor for the number of epochs and number of models for each bracket.</p></td>
    </tr>
    <tr>
      <th>hyperband_iterations</th>
      <td><p>Int &gt;= 1. The number of times to iterate over the full Hyperband algorithm.
One iteration will run approximately ```max_epochs * (math.log(max_epochs, factor) ** 2)``` cumulative epochs
across all trials. It is recommended to set this to as high a value as is within your resource budget.</p></td>
    </tr>
    <tr>
      <th>seed</th>
      <td><p>Int. Random seed.</p></td>
    </tr>
    <tr>
      <th>tune_new_entries</th>
      <td><p>Whether hyperparameter entries that are requested by the hypermodel 
but that were not specified in hyperparameters should be added to the search space, or not. 
If not, then the default value for these parameters will be used.</p></td>
    </tr>
    <tr>
      <th>allow_new_entries</th>
      <td><p>Whether the hypermodel is allowed to request hyperparameter entries not listed in 
`hyperparameters`. **kwargs: Keyword arguments relevant to all `Tuner` subclasses. Please see the docstring for `Tuner`.</p></td>
    </tr>
    <tr>
      <th>distribution_strategy</th>
      <td><p>Scale up from running single-threaded locally to running on dozens or 
hundreds of workers in parallel. Distributed Keras Tuner uses a chief-worker model. The chief runs a 
service to which the workers report results and query for the hyperparameters to try next. The chief 
should be run on a single-threaded CPU instance (or alternatively as a separate process on 
one of the workers). Keras Tuner also supports data parallelism via tf.distribute. 
Data parallelism and distributed tuning can be combined. For example, if you have 10 workers 
with 4 GPUs on each worker, you can run 10 parallel trials with each trial training on 4 GPUs 
by using tf.distribute.MirroredStrategy. You can also run each trial on TPUs 
via tf.distribute.experimental.TPUStrategy. Currently tf.distribute.MultiWorkerMirroredStrategy 
is not supported, but support for this is on the roadmap.</p></td>
    </tr>
    <tr>
      <th>directory</th>
      <td><p>The dir where training logs are stored</p></td>
    </tr>
    <tr>
      <th>project_name</th>
      <td><p>Detailed logs, checkpoints, etc, in the folder my_dir/helloworld, i.e. 
directory/project_name.</p></td>
    </tr>
    <tr>
      <th>...</th>
      <td><p>Some additional arguments</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>a hyperparameter tuner object Hyperband</p>
    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>Reference: Li, Lisha, and Kevin Jamieson. ["Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization." Journal of Machine Learning Research 18 (2018): 1-52]( http://jmlr.org/papers/v18/16-558.html). # Arguments hypermodel: Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance). objective: String. Name of model metric to minimize or maximize, e.g. "val_accuracy". max_epochs: Int. The maximum number of epochs to train one model. It is recommended to set this to a value slightly higher than the expected time to convergence for your largest Model, and to use early stopping during training (for example, via `tf.keras.callbacks.EarlyStopping`). factor: Int. Reduction factor for the number of epochs and number of models for each bracket. hyperband_iterations: Int &gt;= 1. The number of times to iterate over the full Hyperband algorithm. One iteration will run approximately `max_epochs * (math.log(max_epochs, factor) ** 2)` cumulative epochs across all trials. It is recommended to set this to as high a value as is within your resource budget. seed: Int. Random seed. hyperparameters: HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries: Whether hyperparameter entries that are requested by the hypermodel but that were not specified in `hyperparameters` should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries: Whether the hypermodel is allowed to request hyperparameter entries not listed in `hyperparameters`. **kwargs: Keyword arguments relevant to all `Tuner` subclasses. Please see the docstring for `Tuner`.</p>
    <h2 class="hasAnchor" id="reference"><a class="anchor" href="#reference"></a>Reference</h2>

    

<p>Li, Lisha, and Kevin Jamieson. ["Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization." Journal of Machine Learning Research 18 (2018): 1-52]( http://jmlr.org/papers/v18/16-558.html).</p>

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Turgut Abdullayev.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


